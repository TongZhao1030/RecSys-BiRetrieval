"""
è›‹ç™½è´¨-åˆ†å­åŒå‘æ£€ç´¢ Demo
Gradio Web UI
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import os
import gradio as gr
from transformers import AutoModel, AutoTokenizer
from datasets import load_from_disk

# é…ç½®
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
PATH_MODEL = "dual_tower_final.pth"
PATH_SAPROT = "/share/home/zhangchiLab/duyinuo/models/westlake-repl_SaProt_650M_AF2"
PATH_CHEMBERTA = "/share/home/zhangchiLab/duyinuo/models/seyonec_ChemBERTa-zinc-base-v1"
PATH_DATA = "/share/home/zhangchiLab/duyinuo/data/vladak_bindingdb"
TOP_K = 10

def resolve_split_dataset_path(data_path, split):
    if split:
        candidate = os.path.join(data_path, split)
        if os.path.isdir(candidate):
            return candidate
    for default_split in ("train", "test", "valid", "validation"):
        candidate = os.path.join(data_path, default_split)
        if os.path.isdir(candidate):
            return candidate
    return data_path

def infer_text_column(cols, *, kind):
    cols = list(cols or [])
    if kind == "protein":
        preferred = ["Protein Sequence", "protein_sequence", "protein", "target_sequence", "target"]
        needles = ("protein", "target", "sequence")
    else:
        preferred = ["Molecule Sequence", "molecule_sequence", "smiles", "ligand", "molecule", "drug"]
        needles = ("smiles", "ligand", "molecule", "drug")
    for c in preferred:
        if c in cols:
            return c
    for c in cols:
        low = c.lower()
        if any(n in low for n in needles):
            return c
    return None

# æ¨¡å‹å®šä¹‰
class DualTowerModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.prot_model = AutoModel.from_pretrained(PATH_SAPROT, trust_remote_code=True)
        self.mol_model = AutoModel.from_pretrained(PATH_CHEMBERTA)
        
        prot_hidden = self.prot_model.config.hidden_size
        mol_hidden = 768
        hidden_dim = 1024
        embedding_dim = 256

        self.prot_layernorm = nn.LayerNorm(prot_hidden)
        self.mol_layernorm = nn.LayerNorm(mol_hidden)

        self.prot_proj = nn.Sequential(
            nn.Linear(prot_hidden, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, embedding_dim)
        )
        
        self.mol_proj = nn.Sequential(
            nn.Linear(mol_hidden, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, embedding_dim)
        )
        
        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))

# å…¨å±€å˜é‡
model = None
mol_tokenizer = None
prot_tokenizer = None
mol_database = []
prot_database = []
mol_vectors = None
prot_vectors = None

def load_model():
    """åŠ è½½æ¨¡å‹å’Œæ•°æ®"""
    global model, mol_tokenizer, prot_tokenizer
    global mol_database, prot_database, mol_vectors, prot_vectors
    
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    model = DualTowerModel().to(DEVICE)
    state_dict = torch.load(PATH_MODEL, map_location=DEVICE, weights_only=False)
    new_state_dict = {k.replace("module.", ""): v for k, v in state_dict.items()}
    model.load_state_dict(new_state_dict)
    model.eval()
    print(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {DEVICE}")
    
    mol_tokenizer = AutoTokenizer.from_pretrained(PATH_CHEMBERTA)
    prot_tokenizer = AutoTokenizer.from_pretrained(PATH_SAPROT, trust_remote_code=True)
    
    # åŠ è½½æ•°æ®åº“
    print("æ­£åœ¨æ„å»ºæ£€ç´¢æ•°æ®åº“...")
    dataset = load_from_disk(resolve_split_dataset_path(PATH_DATA, "train"))
    protein_col = infer_text_column(dataset.column_names, kind="protein")
    molecule_col = infer_text_column(dataset.column_names, kind="molecule")
    if protein_col is None or molecule_col is None:
        raise ValueError(f"æ— æ³•æ¨æ–­åˆ—åï¼›ç°æœ‰åˆ—: {dataset.column_names}")
    
    seen_prots = set()
    seen_mols = set()
    
    for idx in range(min(len(dataset), 5000)):  # é™åˆ¶æ•°æ®åº“å¤§å°
        prot = dataset[idx][protein_col]
        mol = dataset[idx][molecule_col]
        
        if prot not in seen_prots:
            seen_prots.add(prot)
            prot_database.append(prot)
        
        if mol not in seen_mols:
            seen_mols.add(mol)
            mol_database.append(mol)
    
    print(f"æ•°æ®åº“: {len(prot_database)} è›‹ç™½è´¨, {len(mol_database)} åˆ†å­")
    
    # é¢„è®¡ç®—å‘é‡
    print("æ­£åœ¨é¢„è®¡ç®—å‘é‡...")
    mol_vectors = encode_molecules_batch(mol_database)
    prot_vectors = encode_proteins_batch(prot_database)
    print("å‘é‡è®¡ç®—å®Œæˆ")

def encode_protein(seq):
    """ç¼–ç å•ä¸ªè›‹ç™½è´¨"""
    formatted = " ".join([aa + "#" for aa in seq])
    inputs = prot_tokenizer(formatted, return_tensors="pt", padding=True, 
                            truncation=True, max_length=512).to(DEVICE)
    with torch.no_grad():
        out = model.prot_model(input_ids=inputs['input_ids'], 
                               attention_mask=inputs['attention_mask'])
        mask = inputs['attention_mask'].unsqueeze(-1).expand(out.last_hidden_state.size()).float()
        emb = torch.sum(out.last_hidden_state * mask, dim=1) / torch.clamp(mask.sum(1), min=1e-9)
        emb = model.prot_layernorm(emb)
        vec = model.prot_proj(emb)
        vec = F.normalize(vec, p=2, dim=1)
    return vec

def encode_molecule(smiles):
    """ç¼–ç å•ä¸ªåˆ†å­"""
    inputs = mol_tokenizer(smiles, return_tensors="pt", padding=True,
                           truncation=True, max_length=128).to(DEVICE)
    with torch.no_grad():
        out = model.mol_model(input_ids=inputs['input_ids'],
                              attention_mask=inputs['attention_mask'])
        mask = inputs['attention_mask'].unsqueeze(-1).expand(out.last_hidden_state.size()).float()
        emb = torch.sum(out.last_hidden_state * mask, dim=1) / torch.clamp(mask.sum(1), min=1e-9)
        emb = model.mol_layernorm(emb)
        vec = model.mol_proj(emb)
        vec = F.normalize(vec, p=2, dim=1)
    return vec

def encode_proteins_batch(prot_list, batch_size=16):
    """æ‰¹é‡ç¼–ç è›‹ç™½è´¨"""
    all_vecs = []
    for i in range(0, len(prot_list), batch_size):
        batch = prot_list[i:i+batch_size]
        formatted = [" ".join([aa + "#" for aa in seq]) for seq in batch]
        inputs = prot_tokenizer(formatted, return_tensors="pt", padding=True,
                                truncation=True, max_length=512).to(DEVICE)
        with torch.no_grad():
            out = model.prot_model(input_ids=inputs['input_ids'],
                                   attention_mask=inputs['attention_mask'])
            mask = inputs['attention_mask'].unsqueeze(-1).expand(out.last_hidden_state.size()).float()
            emb = torch.sum(out.last_hidden_state * mask, dim=1) / torch.clamp(mask.sum(1), min=1e-9)
            emb = model.prot_layernorm(emb)
            vec = model.prot_proj(emb)
            vec = F.normalize(vec, p=2, dim=1)
        all_vecs.append(vec)
    return torch.cat(all_vecs, dim=0)

def encode_molecules_batch(mol_list, batch_size=32):
    """æ‰¹é‡ç¼–ç åˆ†å­"""
    all_vecs = []
    for i in range(0, len(mol_list), batch_size):
        batch = mol_list[i:i+batch_size]
        inputs = mol_tokenizer(batch, return_tensors="pt", padding=True,
                               truncation=True, max_length=128).to(DEVICE)
        with torch.no_grad():
            out = model.mol_model(input_ids=inputs['input_ids'],
                                  attention_mask=inputs['attention_mask'])
            mask = inputs['attention_mask'].unsqueeze(-1).expand(out.last_hidden_state.size()).float()
            emb = torch.sum(out.last_hidden_state * mask, dim=1) / torch.clamp(mask.sum(1), min=1e-9)
            emb = model.mol_layernorm(emb)
            vec = model.mol_proj(emb)
            vec = F.normalize(vec, p=2, dim=1)
        all_vecs.append(vec)
    return torch.cat(all_vecs, dim=0)

def search_molecules(protein_seq, top_k=TOP_K):
    """ç»™å®šè›‹ç™½è´¨ï¼Œæ£€ç´¢åˆ†å­"""
    if not protein_seq.strip():
        return "è¯·è¾“å…¥è›‹ç™½è´¨åºåˆ—"
    
    try:
        query_vec = encode_protein(protein_seq.strip().upper())
        scores = torch.matmul(query_vec, mol_vectors.T).squeeze().cpu().numpy()
        top_indices = np.argsort(scores)[::-1][:top_k]
        
        results = []
        for rank, idx in enumerate(top_indices, 1):
            results.append(f"**{rank}.** `{mol_database[idx]}`  \nç›¸ä¼¼åº¦: {scores[idx]:.4f}\n")
        
        return "\n".join(results)
    except Exception as e:
        return f"é”™è¯¯: {str(e)}"

def search_proteins(smiles, top_k=TOP_K):
    """ç»™å®šåˆ†å­ï¼Œæ£€ç´¢è›‹ç™½è´¨"""
    if not smiles.strip():
        return "è¯·è¾“å…¥ SMILES"
    
    try:
        query_vec = encode_molecule(smiles.strip())
        scores = torch.matmul(query_vec, prot_vectors.T).squeeze().cpu().numpy()
        top_indices = np.argsort(scores)[::-1][:top_k]
        
        results = []
        for rank, idx in enumerate(top_indices, 1):
            prot_seq = prot_database[idx]
            display_seq = prot_seq[:60] + "..." if len(prot_seq) > 60 else prot_seq
            results.append(f"**{rank}.** `{display_seq}`  \nç›¸ä¼¼åº¦: {scores[idx]:.4f}\n")
        
        return "\n".join(results)
    except Exception as e:
        return f"é”™è¯¯: {str(e)}"

def compute_similarity(protein_seq, smiles):
    """è®¡ç®—å•å¯¹ç›¸ä¼¼åº¦"""
    if not protein_seq.strip() or not smiles.strip():
        return "è¯·è¾“å…¥è›‹ç™½è´¨åºåˆ—å’Œ SMILES"
    
    try:
        prot_vec = encode_protein(protein_seq.strip().upper())
        mol_vec = encode_molecule(smiles.strip())
        similarity = torch.matmul(prot_vec, mol_vec.T).item()
        return f"## ç›¸ä¼¼åº¦å¾—åˆ†: {similarity:.4f}"
    except Exception as e:
        return f"é”™è¯¯: {str(e)}"

# ç¤ºä¾‹æ•°æ®
EXAMPLE_PROTEIN = "MIKSALLVLEDGTQFHGRAIGATGSAVGEVVFNTSMTGYQEILTDPSYSRQIVTLTYPHIGNVGTNDADEESSQVHAQGLVIRDLPLIASNFRNTEDLSSYLKRHNIVAIADIDTRKLTRLLREKGAQNGCIIAGDNPDAALALEKARAFPGLNGMDLAKEVTTAEAYSWTQGSWTLTGGLPEAKKEDELPFHVVAYDFGAKRNILRMLVDRGCRLTIVPAQTSAEDVLKMNPDGIFLSNGPGDPAPCDYAITAIQKFLETDIPVFGICLGHQLLALASGAKTVKMKFGHHGGNHPVKDVEKNVVMITAQNHGFAVDEATLPANLRVTHKSLFDGTLQGIHRTDKPAFSFQGHPEASPGPHDAAPLFDHFIELIEQYRKTAK"
EXAMPLE_SMILES = "O[C@@H]1[C@@H](COP(O)(O)=O)O[C@H]([C@@H]1O)n1cnc2c3nccn3cnc12"

# æ„å»ºç•Œé¢
with gr.Blocks(title="è›‹ç™½è´¨-åˆ†å­åŒå‘æ£€ç´¢", theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # ğŸ§¬ è›‹ç™½è´¨-åˆ†å­åŒå‘æ£€ç´¢ç³»ç»Ÿ
    
    åŸºäºåŒå¡”å¯¹æ¯”å­¦ä¹ çš„æ£€ç´¢æ¨¡å‹ï¼Œæ”¯æŒï¼š
    - **Protein â†’ Molecule**ï¼šç»™å®šé¶ç‚¹è›‹ç™½ï¼Œæ£€ç´¢å€™é€‰ç»“åˆåˆ†å­
    - **Molecule â†’ Protein**ï¼šç»™å®šå°åˆ†å­ï¼Œè¯†åˆ«æ½œåœ¨é¶ç‚¹è›‹ç™½
    """)
    
    with gr.Tab("ğŸ”¬ è›‹ç™½è´¨ â†’ åˆ†å­"):
        gr.Markdown("è¾“å…¥è›‹ç™½è´¨åºåˆ—ï¼Œæ£€ç´¢å¯èƒ½ç»“åˆçš„å°åˆ†å­è¯ç‰©")
        with gr.Row():
            with gr.Column():
                prot_input = gr.Textbox(
                    label="è›‹ç™½è´¨åºåˆ—",
                    placeholder="è¾“å…¥æ°¨åŸºé…¸åºåˆ—ï¼ˆå¦‚ MKTVRQ...ï¼‰",
                    lines=4,
                    value=EXAMPLE_PROTEIN
                )
                search_mol_btn = gr.Button("ğŸ” æ£€ç´¢åˆ†å­", variant="primary")
            with gr.Column():
                mol_output = gr.Markdown(label="æ£€ç´¢ç»“æœ")
        search_mol_btn.click(search_molecules, inputs=prot_input, outputs=mol_output)
    
    with gr.Tab("ğŸ’Š åˆ†å­ â†’ è›‹ç™½è´¨"):
        gr.Markdown("è¾“å…¥åˆ†å­ SMILESï¼Œæ£€ç´¢å¯èƒ½çš„é¶ç‚¹è›‹ç™½")
        with gr.Row():
            with gr.Column():
                mol_input = gr.Textbox(
                    label="åˆ†å­ SMILES",
                    placeholder="è¾“å…¥ SMILESï¼ˆå¦‚ CC(=O)Oc1ccccc1C(=O)Oï¼‰",
                    lines=2,
                    value=EXAMPLE_SMILES
                )
                search_prot_btn = gr.Button("ğŸ” æ£€ç´¢è›‹ç™½è´¨", variant="primary")
            with gr.Column():
                prot_output = gr.Markdown(label="æ£€ç´¢ç»“æœ")
        search_prot_btn.click(search_proteins, inputs=mol_input, outputs=prot_output)
    
    with gr.Tab("âš¡ ç›¸ä¼¼åº¦è®¡ç®—"):
        gr.Markdown("è®¡ç®—å•å¯¹è›‹ç™½è´¨-åˆ†å­çš„ç›¸ä¼¼åº¦å¾—åˆ†")
        with gr.Row():
            with gr.Column():
                pair_prot = gr.Textbox(label="è›‹ç™½è´¨åºåˆ—", lines=3, value=EXAMPLE_PROTEIN)
                pair_mol = gr.Textbox(label="åˆ†å­ SMILES", lines=1, value=EXAMPLE_SMILES)
                calc_btn = gr.Button("âš¡ è®¡ç®—ç›¸ä¼¼åº¦", variant="primary")
            with gr.Column():
                sim_output = gr.Markdown()
        calc_btn.click(compute_similarity, inputs=[pair_prot, pair_mol], outputs=sim_output)
    
    gr.Markdown("""
    ---
    **æ¨¡å‹ä¿¡æ¯**ï¼šSaProt (650M) + ChemBERTa | å¯¹æ¯”å­¦ä¹  | BindingDB æ•°æ®é›†
    """)

if __name__ == "__main__":
    load_model()
    demo.launch(server_name="0.0.0.0", server_port=7860, share=True)
